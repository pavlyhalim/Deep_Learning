{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWGUYEjVdTJU"
   },
   "source": [
    "# Implementing ROUGE-L Score for LLM Summarization Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5dNjncUGq5k"
   },
   "source": [
    "Your name : [Pavly Halim]\n",
    "\n",
    "Net id : [poh2005]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mt4J3AdRdb6V"
   },
   "source": [
    "Total Points: 25\n",
    "\n",
    "### Background\n",
    "\n",
    "The ROUGE-L score is a critical metric for evaluating text summarization quality, measuring the longest common subsequence (LCS) between a generated summary and reference summaries. This assignment will guide you through implementing and using this metric to evaluate LLM-generated summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qshkp9I1e6h2"
   },
   "source": [
    "### Assignment Objectives\n",
    "\n",
    "*   Understand and implement the ROUGE-L scoring metric\n",
    "*   Work with real-world summarization data\n",
    "*   Gain practical experience with LLM APIs\n",
    "*   Apply text preprocessing techniques\n",
    "*   Evaluate machine-generated summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aq6M4eDYfZ2n"
   },
   "source": [
    "### Tasks and Scoring Rubric\n",
    "#### Part 1: Data Preparation (5 points)\n",
    "\n",
    "- Load the CNN/DailyMail dataset using the Hugging Face datasets library (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yzVzTW8RftK5"
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YXqE9PunfAly"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test[:10]\")  # Loading 10 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoWGlue1f--a"
   },
   "source": [
    "- Implement text preprocessing functions (3 points)\n",
    "  - Basic text cleaning (1 point)\n",
    "  - Remove special characters (0.5 point)\n",
    "  - Handle contractions and whitespace (0.5 point)\n",
    "\n",
    "- Text tokenization and normalization (1.5 points)\n",
    "  - NLTK tokenization with fallback (0.5 point)\n",
    "  - Case normalization (0.5 point)\n",
    "  - Word stemming using PorterStemmer (0.5 point)\n",
    "\n",
    "- Error handling and robustness (0.5 point)\n",
    "  - Proper error handling for all preprocessing steps\n",
    "  - Appropriate fallback mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFZCnkOlgEEg"
   },
   "outputs": [],
   "source": [
    "!pip install nltk>=3.6.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "s5a01YCTgMOZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/poh2005/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/poh2005/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/poh2005/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK resources downloaded successfully!\n",
      "NLTK setup completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download all required NLTK resources\n",
    "def setup_nltk():\n",
    "    \"\"\"Download required NLTK resources\"\"\"\n",
    "    try:\n",
    "        # Download both punkt and punkt_tab\n",
    "        nltk.download('punkt')\n",
    "\n",
    "        # Additional recommended resources for robust tokenization\n",
    "        nltk.download('averaged_perceptron_tagger')\n",
    "        nltk.download('wordnet')\n",
    "\n",
    "        print(\"NLTK resources downloaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading NLTK resources: {e}\")\n",
    "        raise\n",
    "\n",
    "# Run setup before defining the class\n",
    "try:\n",
    "    setup_nltk()\n",
    "    print(\"NLTK setup completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to setup NLTK: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IktuFeP5tyhg"
   },
   "outputs": [],
   "source": [
    "!pip install num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "tt_j1lJLhCnS"
   },
   "outputs": [],
   "source": [
    "from num2words import num2words\n",
    "# import num2words\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        # Add stemmer\n",
    "        self.stemmer = PorterStemmer()\n",
    "        try:\n",
    "            word_tokenize(\"Test sentence.\")\n",
    "        except LookupError as e:\n",
    "            print(\"NLTK resources not found. Running setup again...\")\n",
    "            setup_nltk()\n",
    "\n",
    "        self.contractions = {\n",
    "            \"n't\": \" not\",\n",
    "            \"'ll\": \" will\",\n",
    "            \"'ve\": \" have\",\n",
    "            \"'re\": \" are\",\n",
    "            \"'m\": \" am\",\n",
    "            \"'s\": \" is\"\n",
    "        }\n",
    "\n",
    "    # Keep existing expand_contractions method\n",
    "    def expand_contractions(self, text):\n",
    "        for contraction, expansion in self.contractions.items():\n",
    "            text = text.replace(contraction, expansion)\n",
    "        return text\n",
    "\n",
    "    def remove_special_characters(self, text):\n",
    "        \"\"\"\n",
    "        More careful handling of quotation marks and numbers\n",
    "        \"\"\"\n",
    "        # Keep content in parentheses\n",
    "        text = re.sub(r'\\(([^)]+)\\)', r' \\1 ', text)\n",
    "\n",
    "        # Remove URLs and emails\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "        # Convert numbers to standard form\n",
    "        text = re.sub(r'\\d+', lambda m: num2words(int(m.group())), text)\n",
    "\n",
    "        # More careful with quotes and special characters\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s.,!?\"\\'-]', ' ', text)\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def tokenize_text(self, text):\n",
    "        \"\"\"\n",
    "        Updated tokenization to better match rouge-score\n",
    "        \"\"\"\n",
    "        try:\n",
    "            tokens = word_tokenize(text)\n",
    "            # Keep punctuation tokens that rouge-score considers\n",
    "            return [token for token in tokens if token not in {'``', \"''\"}]\n",
    "        except LookupError:\n",
    "            print(\"Warning: Using basic tokenization as fallback\")\n",
    "            return text.split()\n",
    "\n",
    "    def normalize_case(self, tokens):\n",
    "        \"\"\"\n",
    "        Add stemming to handle word variations\n",
    "        \"\"\"\n",
    "        # First normalize case\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        # Then stem the tokens\n",
    "        return [self.stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        # Extract acronyms before processing\n",
    "        acronyms = re.findall(r'\\b[A-Z]{2,}\\b', text)\n",
    "\n",
    "        # Normal processing\n",
    "        text = self.expand_contractions(text)\n",
    "        text = self.remove_special_characters(text)\n",
    "        tokens = self.tokenize_text(text)\n",
    "\n",
    "        # Add acronyms back (both forms)\n",
    "        tokens.extend([acr.lower() for acr in acronyms])\n",
    "\n",
    "        tokens = self.normalize_case(tokens)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "SzySDwH0gWjP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK resources not found. Running setup again...\n",
      "NLTK resources downloaded successfully!\n",
      "Original text:\n",
      "Hello! This is a sample text w/ special chars... Check it out @ http://example.com\n",
      "Warning: Using basic tokenization as fallback\n",
      "\n",
      "Processed tokens:\n",
      "['hello!', 'thi', 'is', 'a', 'sampl', 'text', 'w', 'special', 'chars...', 'check', 'it', 'out']\n",
      "\n",
      "Processing details:\n",
      "Number of tokens: 12\n",
      "Tokens after normalization and stemming:\n",
      "1. hello!\n",
      "2. thi\n",
      "3. is\n",
      "4. a\n",
      "5. sampl\n",
      "6. text\n",
      "7. w\n",
      "8. special\n",
      "9. chars...\n",
      "10. check\n",
      "11. it\n",
      "12. out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/poh2005/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/poh2005/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/poh2005/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Test with sample text\n",
    "sample_text = \"Hello! This is a sample text w/ special chars... Check it out @ http://example.com\"\n",
    "\n",
    "try:\n",
    "    # Print original text for comparison\n",
    "    print(\"Original text:\")\n",
    "    print(sample_text)\n",
    "    \n",
    "    # Generate processed tokens\n",
    "    processed_tokens = preprocessor.preprocess(sample_text)\n",
    "    \n",
    "    # Print processed tokens\n",
    "    print(\"\\nProcessed tokens:\")\n",
    "    print(processed_tokens)\n",
    "    \n",
    "    # Print additional processing details\n",
    "    print(\"\\nProcessing details:\")\n",
    "    print(f\"Number of tokens: {len(processed_tokens)}\")\n",
    "    print(f\"Tokens after normalization and stemming:\")\n",
    "    for i, token in enumerate(processed_tokens, 1):\n",
    "        print(f\"{i}. {token}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing text: {e}\")\n",
    "    raise  # Re-raise the exception for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnQ8MxDmhcvu"
   },
   "source": [
    "#### Part 2: Generate Summaries using OpenAI API (5 points)\n",
    "\n",
    "- Set up OpenAI API authentication (1 point)\n",
    "- Implement API calling function (2 points)\n",
    "- Handle API responses and errors (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEY4njqfng7o"
   },
   "outputs": [],
   "source": [
    "!pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "2ZxhNmg3hg0V"
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found in environment variables\")\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "def get_summary(text):\n",
    "    \"\"\"\n",
    "    Generate a summary using Google's Gemini API\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to summarize\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated summary or None if there's an error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Configure the model\n",
    "        model = genai.GenerativeModel('gemini-pro')\n",
    "        \n",
    "        # Prepare the prompt\n",
    "        prompt = f\"\"\"Summarize the following text concisely, capturing the main points:\n",
    "\n",
    "{text}\n",
    "\n",
    "Provide a clear and focused summary.\"\"\"\n",
    "\n",
    "        # Generate response with retry mechanism\n",
    "        max_retries = 3\n",
    "        retry_delay = 2\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = model.generate_content(\n",
    "                    prompt,\n",
    "                    generation_config={\n",
    "                        'temperature': 0.3,\n",
    "                        'top_p': 0.8,\n",
    "                        'top_k': 40,\n",
    "                        'max_output_tokens': 1024,\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                if response.parts[0].text:\n",
    "                    return response.parts[0].text.strip()\n",
    "                else:\n",
    "                    print(f\"Response was empty on attempt {attempt + 1}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "                    time.sleep(retry_delay)\n",
    "                    continue\n",
    "                else:\n",
    "                    raise\n",
    "                    \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_summary: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mACDM53eiJlz"
   },
   "source": [
    "#### Part 3: ROUGE-L and ROUGE-LSum Implementation (15 points)\n",
    "\n",
    "3.1 Basic ROUGE-L Implementation (6 points)\n",
    "\n",
    "  3.1.1 LCS table implementation (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "fQ9ga5ZuiQ8Q"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "def get_lcs_table(ref_tokens: List[str], pred_tokens: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the Longest Common Subsequence table between reference and prediction tokens.\n",
    "    \n",
    "    Args:\n",
    "        ref_tokens: List of tokens from the reference text\n",
    "        pred_tokens: List of tokens from the predicted text\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: 2D array containing LCS lengths for all substrings\n",
    "    \"\"\"\n",
    "    m, n = len(ref_tokens), len(pred_tokens)\n",
    "    lcs_table = np.zeros((m + 1, n + 1), dtype=np.int32)\n",
    "    \n",
    "    # Fill the LCS table using dynamic programming\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if ref_tokens[i-1] == pred_tokens[j-1]:\n",
    "                lcs_table[i][j] = lcs_table[i-1][j-1] + 1\n",
    "            else:\n",
    "                lcs_table[i][j] = max(lcs_table[i-1][j], lcs_table[i][j-1])\n",
    "    \n",
    "    return lcs_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wlKvEeGiaEr"
   },
   "source": [
    "3.1.2 Implement ROUGE-L score calculation (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "pcEoAfLaicQG"
   },
   "outputs": [],
   "source": [
    "def compute_rouge_l(reference: List[str], prediction: List[str], beta: float = 1.2) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute ROUGE-L scores between reference and prediction texts.\n",
    "    \n",
    "    Args:\n",
    "        reference: List of tokens from reference text\n",
    "        prediction: List of tokens from predicted text\n",
    "        beta: Weight of recall relative to precision\n",
    "    \n",
    "    Returns:\n",
    "        Dict containing precision, recall, and F1 scores\n",
    "    \"\"\"\n",
    "    if not reference or not prediction:\n",
    "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "    \n",
    "    # Get LCS table\n",
    "    lcs_table = get_lcs_table(reference, prediction)\n",
    "    \n",
    "    # Get length of LCS from the bottom-right cell\n",
    "    lcs_length = lcs_table[-1, -1]\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    precision = lcs_length / len(prediction) if prediction else 0.0\n",
    "    recall = lcs_length / len(reference) if reference else 0.0\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    if precision == 0.0 and recall == 0.0:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        f1 = ((1 + beta**2) * precision * recall) / (beta**2 * precision + recall)\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCJiGUVLkyFi"
   },
   "source": [
    "3.2 Implement Rouge-LSum (5 points)\n",
    "\n",
    "3.2.1 Split tokens into sentences (1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "loEvaWhKie_E"
   },
   "outputs": [],
   "source": [
    "def split_into_sentences(tokens: List[str]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Split tokens into sentences based on punctuation markers.\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of tokens to split into sentences\n",
    "    \n",
    "    Returns:\n",
    "        List of sentences, where each sentence is a list of tokens\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        current_sentence.append(token)\n",
    "        \n",
    "        # Check for sentence-ending punctuation\n",
    "        if token in ['.', '!', '?']:\n",
    "            if current_sentence:\n",
    "                sentences.append(current_sentence)\n",
    "                current_sentence = []\n",
    "    \n",
    "    # Add any remaining tokens as a sentence\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGmdTgv-lHAa"
   },
   "source": [
    "3.2.2 ROUGE-LSum (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "k285PG8xlLH7"
   },
   "outputs": [],
   "source": [
    "def compute_rouge_lsum(reference: List[str], prediction: List[str], beta: float = 1.2) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute ROUGE-LSum score between reference and prediction texts.\n",
    "    This implementation handles multi-sentence summaries by computing LCS\n",
    "    for each reference sentence separately.\n",
    "    \n",
    "    Args:\n",
    "        reference: List of tokens from reference text\n",
    "        prediction: List of tokens from predicted text\n",
    "        beta: Weight of recall relative to precision\n",
    "    \n",
    "    Returns:\n",
    "        Dict containing precision, recall, and F1 scores\n",
    "    \"\"\"\n",
    "    if not reference or not prediction:\n",
    "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "    \n",
    "    try:\n",
    "        # Split into sentences\n",
    "        ref_sentences = split_into_sentences(reference)\n",
    "        pred_sentences = split_into_sentences(prediction)\n",
    "        \n",
    "        # Calculate LCS for each reference sentence\n",
    "        total_lcs_length = 0\n",
    "        for ref_sent in ref_sentences:\n",
    "            # Find the best matching prediction sentence\n",
    "            max_lcs = 0\n",
    "            for pred_sent in pred_sentences:\n",
    "                lcs_table = get_lcs_table(ref_sent, pred_sent)\n",
    "                max_lcs = max(max_lcs, lcs_table[-1, -1])\n",
    "            total_lcs_length += max_lcs\n",
    "        \n",
    "        # Calculate final scores\n",
    "        precision = total_lcs_length / len(prediction) if prediction else 0.0\n",
    "        recall = total_lcs_length / len(reference) if reference else 0.0\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        if precision == 0.0 and recall == 0.0:\n",
    "            f1 = 0.0\n",
    "        else:\n",
    "            f1 = ((1 + beta**2) * precision * recall) / (beta**2 * precision + recall)\n",
    "        \n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in ROUGE-LSum computation: {e}\")\n",
    "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xaPygWwlVda"
   },
   "source": [
    "3.3 Testing Implementation (4 points)\n",
    "\n",
    "Test ROUGE implementation using CNN/DailyMail dataset and OpenAI summarization\n",
    "Points for:\n",
    "\n",
    "- Dataset integration (0.5 point)\n",
    "  - Successfully load CNN/DailyMail dataset\n",
    "  - Handle data extraction properly\n",
    "\n",
    "- Preprocessing implementation (0.5 point)\n",
    "  - Implement text cleaning and tokenization\n",
    "  - Handle preprocessing edge cases\n",
    "\n",
    "- API integration (0.5 point)\n",
    "  - Implement OpenAI API calls\n",
    "  - Handle API errors appropriately\n",
    "\n",
    "- Official library comparison (1.5 points)\n",
    "  - Install and integrate rouge-score library (0.5 point)\n",
    "  - Compare custom scores with official library scores (0.5 point)\n",
    "  - Analyze and document differences (max difference < 5%) (0.5 point)\n",
    "\n",
    "- Score calculation and results analysis (1 point)\n",
    "  - Calculate and display both custom and official ROUGE scores\n",
    "  - Provide clear comparison of results\n",
    "  - Understand any significant differences and potential improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "b2eavIb5qcAV"
   },
   "outputs": [],
   "source": [
    "# First install the rouge-score library\n",
    "# !pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "GS_KlUhOlXhD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/poh2005/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def test_rouge_with_dataset(sample_idx: int):\n",
    "    \"\"\"\n",
    "    Test ROUGE implementation using a single article from CNN/DailyMail dataset\n",
    "    \n",
    "    Args:\n",
    "        sample_idx: Index of the article to test\n",
    "    \"\"\"\n",
    "    # Initialize preprocessor and official scorer\n",
    "    preprocessor = TextPreprocessor()\n",
    "    official_scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "    print(f\"Testing ROUGE scores with article index {sample_idx} from CNN/DailyMail dataset...\")\n",
    "\n",
    "    try:\n",
    "        # Get the article\n",
    "        article = dataset[sample_idx]\n",
    "\n",
    "        # Get original article and reference summary\n",
    "        original_text = article['article']\n",
    "        reference_summary = article['highlights']\n",
    "\n",
    "        print(f\"\\nOriginal text length: {len(original_text)}\")\n",
    "        print(f\"Reference summary length: {len(reference_summary)}\")\n",
    "\n",
    "        # Generate summary using Gemini\n",
    "        generated_summary = get_summary(original_text)\n",
    "        if not generated_summary:\n",
    "            print(\"Error: Could not generate summary\")\n",
    "            return None\n",
    "\n",
    "        # Preprocess texts for custom implementation\n",
    "        ref_tokens = preprocessor.preprocess(reference_summary)\n",
    "        pred_tokens = preprocessor.preprocess(generated_summary)\n",
    "\n",
    "        # Calculate custom ROUGE scores\n",
    "        rouge_l_scores = compute_rouge_l(ref_tokens, pred_tokens)\n",
    "        rouge_lsum_scores = compute_rouge_lsum(ref_tokens, pred_tokens)\n",
    "\n",
    "        # Calculate official ROUGE scores\n",
    "        official_scores = official_scorer.score(reference_summary, generated_summary)\n",
    "\n",
    "        # Calculate differences\n",
    "        diff_precision = abs(rouge_l_scores['precision'] - official_scores['rougeL'].precision)\n",
    "        diff_recall = abs(rouge_l_scores['recall'] - official_scores['rougeL'].recall)\n",
    "        diff_f1 = abs(rouge_l_scores['f1'] - official_scores['rougeL'].fmeasure)\n",
    "        max_diff = max(diff_precision, diff_recall, diff_f1)\n",
    "\n",
    "        # Print detailed results\n",
    "        print(f\"\\nArticle Results:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(\"\\nReference Summary:\")\n",
    "        print(reference_summary[:200] + \"...\" if len(reference_summary) > 200 else reference_summary)\n",
    "        print(\"\\nGenerated Summary:\")\n",
    "        print(generated_summary[:200] + \"...\" if len(generated_summary) > 200 else generated_summary)\n",
    "\n",
    "        print(\"\\nCustom ROUGE-L Scores:\")\n",
    "        print(f\"Precision: {rouge_l_scores['precision']:.3f}\")\n",
    "        print(f\"Recall: {rouge_l_scores['recall']:.3f}\")\n",
    "        print(f\"F1: {rouge_l_scores['f1']:.3f}\")\n",
    "\n",
    "        print(\"\\nOfficial ROUGE-L Scores:\")\n",
    "        print(f\"Precision: {official_scores['rougeL'].precision:.3f}\")\n",
    "        print(f\"Recall: {official_scores['rougeL'].recall:.3f}\")\n",
    "        print(f\"F1: {official_scores['rougeL'].fmeasure:.3f}\")\n",
    "\n",
    "        print(\"\\nCustom ROUGE-LSum Scores:\")\n",
    "        print(f\"Precision: {rouge_lsum_scores['precision']:.3f}\")\n",
    "        print(f\"Recall: {rouge_lsum_scores['recall']:.3f}\")\n",
    "        print(f\"F1: {rouge_lsum_scores['f1']:.3f}\")\n",
    "\n",
    "        print(\"\\nImplementation Comparison:\")\n",
    "        print(f\"Maximum difference between implementations: {max_diff:.3f}\")\n",
    "        if max_diff < 0.05:\n",
    "            print(\"✓ Custom implementation closely matches the official library (within 5% threshold)\")\n",
    "        else:\n",
    "            print(\"⚠ Custom implementation shows significant differences from the official library\")\n",
    "\n",
    "        return {\n",
    "            'article_id': sample_idx,\n",
    "            'original_length': len(original_text.split()),\n",
    "            'reference_length': len(reference_summary.split()),\n",
    "            'generated_length': len(generated_summary.split()),\n",
    "            'custom_rouge_l': rouge_l_scores,\n",
    "            'custom_rouge_lsum': rouge_lsum_scores,\n",
    "            'official_rouge_l': {\n",
    "                'precision': official_scores['rougeL'].precision,\n",
    "                'recall': official_scores['rougeL'].recall,\n",
    "                'f1': official_scores['rougeL'].fmeasure\n",
    "            }\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing article {sample_idx}: {e}\")\n",
    "        if 'article' in locals():\n",
    "            print(f\"Article structure: {article.keys()}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "cDCUtJ57lZF-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 10\n",
      "Testing articles at indices: [2, 8]\n",
      "\n",
      "Testing article at index 2\n",
      "NLTK resources not found. Running setup again...\n",
      "NLTK resources downloaded successfully!\n",
      "Testing ROUGE scores with article index 2 from CNN/DailyMail dataset...\n",
      "\n",
      "Original text length: 4128\n",
      "Reference summary length: 218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/poh2005/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/poh2005/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/poh2005/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Using basic tokenization as fallback\n",
      "Warning: Using basic tokenization as fallback\n",
      "\n",
      "Article Results:\n",
      "--------------------------------------------------\n",
      "\n",
      "Reference Summary:\n",
      "Mohammad Javad Zarif has spent more time with John Kerry than any other foreign minister .\n",
      "He once participated in a takeover of the Iranian Consulate in San Francisco .\n",
      "The Iranian foreign minister t...\n",
      "\n",
      "Generated Summary:\n",
      "Mohammad Javad Zarif, Iran's Foreign Minister, is known for his diplomacy in nuclear negotiations and his jovial demeanor. Despite his significant role, lesser-known facts about him include:\n",
      "\n",
      "* He twe...\n",
      "\n",
      "Custom ROUGE-L Scores:\n",
      "Precision: 0.130\n",
      "Recall: 0.447\n",
      "F1: 0.223\n",
      "\n",
      "Official ROUGE-L Scores:\n",
      "Precision: 0.159\n",
      "Recall: 0.571\n",
      "F1: 0.248\n",
      "\n",
      "Custom ROUGE-LSum Scores:\n",
      "Precision: 0.206\n",
      "Recall: 0.711\n",
      "F1: 0.355\n",
      "\n",
      "Implementation Comparison:\n",
      "Maximum difference between implementations: 0.124\n",
      "⚠ Custom implementation shows significant differences from the official library\n",
      "Successfully processed article 2\n",
      "\n",
      "Testing article at index 8\n",
      "NLTK resources not found. Running setup again...\n",
      "NLTK resources downloaded successfully!\n",
      "Testing ROUGE scores with article index 8 from CNN/DailyMail dataset...\n",
      "\n",
      "Original text length: 1683\n",
      "Reference summary length: 154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/poh2005/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/poh2005/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/poh2005/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Using basic tokenization as fallback\n",
      "Warning: Using basic tokenization as fallback\n",
      "\n",
      "Article Results:\n",
      "--------------------------------------------------\n",
      "\n",
      "Reference Summary:\n",
      "Once a super typhoon, Maysak is now a tropical storm with 70 mph winds .\n",
      "It could still cause flooding, landslides and other problems in the Philippines .\n",
      "\n",
      "Generated Summary:\n",
      "Tropical storm Maysak (Chedeng) approaches the Philippines, bringing warnings of flash floods and landslides. Despite losing super typhoon status, Maysak still poses a threat with winds over 70 mph. A...\n",
      "\n",
      "Custom ROUGE-L Scores:\n",
      "Precision: 0.110\n",
      "Recall: 0.286\n",
      "F1: 0.172\n",
      "\n",
      "Official ROUGE-L Scores:\n",
      "Precision: 0.137\n",
      "Recall: 0.385\n",
      "F1: 0.202\n",
      "\n",
      "Custom ROUGE-LSum Scores:\n",
      "Precision: 0.123\n",
      "Recall: 0.321\n",
      "F1: 0.194\n",
      "\n",
      "Implementation Comparison:\n",
      "Maximum difference between implementations: 0.099\n",
      "⚠ Custom implementation shows significant differences from the official library\n",
      "Successfully processed article 8\n"
     ]
    }
   ],
   "source": [
    "# Run tests with random samples\n",
    "import random\n",
    "\n",
    "# Get dataset size\n",
    "dataset_size = len(dataset)\n",
    "print(f\"Dataset size: {dataset_size}\")\n",
    "\n",
    "# Generate 2 random indices\n",
    "indices = random.sample(range(dataset_size), 2)\n",
    "print(f\"Testing articles at indices: {indices}\")\n",
    "\n",
    "# Test each randomly selected article\n",
    "results = []\n",
    "for idx in indices:\n",
    "    print(f\"\\nTesting article at index {idx}\")\n",
    "    result = test_rouge_with_dataset(idx)\n",
    "    if result:\n",
    "        results.append(result)\n",
    "        print(f\"Successfully processed article {idx}\")\n",
    "    else:\n",
    "        print(f\"Failed to process article {idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bt6Mxv3RoxHS"
   },
   "source": [
    "### Submission Requirements\n",
    "\n",
    "Submit a Python notebook (.ipynb) containing:\n",
    "\n",
    "1. All implemented functions with appropriate documentation\n",
    "2. Example runs with sample data\n",
    "3. Brief analysis of findings (1-2 paragraphs)\n",
    "\n",
    "#### Notes\n",
    "- Make sure to handle your API keys securely\n",
    "- Include error handling in your implementation\n",
    "- Comment your code appropriately\n",
    "- Include citations for any external resources used\n",
    "\n",
    "### References\n",
    "\n",
    "See, A., Liu, P. J., & Manning, C. D. (2017). Get to the point: Summarization with pointer-generator networks. arXiv preprint arXiv:1704.04368."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzF_36_CSyE6"
   },
   "source": [
    "# Your Analysis Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bt6Mxv3RoxHS"
   },
   "source": [
    "Okay, so I ran some tests comparing our homemade ROUGE scoring system to the official one, using the CNN/DailyMail articles. I found some interesting things.\n",
    "\n",
    "Our homemade system is pretty good – it gives similar results to the official one, but not exactly the same. For two articles I tested (let's call them Article 2 and Article 8), our scores were a little lower across the board (precision, recall, and F1 score). The difference wasn't tiny either; it was bigger than our acceptable 5% margin of error. This tells us that while I am on the right track, there's something subtly different about how we're breaking down the text (tokenization) or calculating the scores.\n",
    "\n",
    "Now, when I used the ROUGE-LSum method, which looks at matches between whole sentences, I got better scores, especially for Article 2. It jumped from an F1 score of 0.223 to 0.355! This makes sense, as ROUGE-LSum is better at handling summaries with multiple sentences. So, that part of our system seems to be working well. However, I did get some warnings about using a basic tokenizer, meaning I could get even better results by improving how I break down the text into individual words.\n",
    "\n",
    "In short: I am close to matching the official ROUGE scores, but there's definitely room for improvement. Our next steps will be refining our text processing (better tokenization) and figuring out exactly why our scores differ from the official version. The goal is to get those differences within that 5% range."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
